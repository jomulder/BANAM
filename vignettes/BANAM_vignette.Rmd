---
title: 'Introduction to BANAM: Bayesian Analysis of the Network Autocorrelation Model'
author: "Stella Maria Marceta, Dino Dittrich, Roger Leender, and Joris Mulder"
date: "29/06/2021"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Put the title of your vignette here}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading required packages
Apart from the `BANAN` package, the packages `sna` and `BFpack` have to be installed to run the three examples provided in this vignette. The two main functions that will be discussed in this vignette are `banam` and `BF`. <!--`banam` is used for estimating (or fitting) a network autocorrelation model, and `BF` is a S3 function from `BFpack` for which is extension has been developed for `BANAM` to compute Bayes factors for Bayesian hypothesis testing and model selection for autocorrelation models by plugging in a fitted `banam` object.-->


```{r, results ='hide', message=FALSE, eval = FALSE}
library(devtools)
devtools::install_github("jomulder/BANAM")
```

```{r Loading required packages, results ='hide', message=FALSE}
library("BANAM")
library("BFpack")
library("sna")
```

## Description of the banam function:
The `banam` function is used for Bayesian estimation and fitting of network autocorrelation models. The standard network autocorrelation model assumes that


supports a single (network) weight matrix as well as multiple weight matrices. For each weight matrix the corresponding network autocorrelation parameter $\rho$ quantifies the degree of a social influence among the actors that can be attributed to the dyadic weights as provided in the weight matrices, possibly while correcting for certain covariates.

in the corresponding network of observations on a variable of interest among related actors in a network. A flat prior, the independence Jeffreys prior, and a normal prior can be specified for the network autocorrelation parameter in the case of a single weight matrix. In the case of multiple weight matrices, a multivariate normal prior can be specified.

## Usage & function arguments:
```{r banam arguments & their defaults overview, eval = FALSE}
banam(
  y,
  X = NULL,
  W,
  prior = "flat",
  priormean = NULL,
  priorsigma = NULL,
  postdraws = 5000,
  burnin = 1000
)
```

* `y` corresponds to	a numeric vector containing the observations of the outcome variable.

* `X` is the design matrix of the predictor variables. If absent, a column of ones is automatically inserted to model the intercept.

* `W` is a weight matrix in the case of a NAM with a single weight matrix or a list of weight matrices if a NAM with multiple weight matrices is estimated. This would, e.g., be done as `banam(y,X,W=list(W1,W2))`.

* `prior` specifies a character string specifying which prior to use in the case of a NAM with a *single weight matrix*. The options are `flat` (flat prior), `IJ` (Independence Jeffrey prior), and `normal` (normal prior).

  + Meaning of priors: 
  - `flat` prior (also called uniform or diffuse prior): assigns equal probabilities to all possible values a parameter could attain (Dittrich et al. 2017, p. 216)
  - `IJ` prior (Independence Jeffreys prior): is an adjusted version of the Jeffreys rule prior that can be used in multiparameter models (see Dittrich et al. 2017 for a more detailed overview)
  - `normal prior`: A normal prior corresponds to an informative prior specifying an (empirically-grounded) prior belief of the true effect size.

* `priormean` corresponds to a scalar (or vector) specifying the prior mean(s) of the network autocorrelation(s). In the univariate case, the default prior mean is 0.36, which is the weakly informative prior mean of Dittrich et al. (2017). In the multivariate case zero prior means are used by default.

* `priorsigma` entails scalar (or matrix) specifying the prior variance (or prior covariance matrix) of the network autocorrelation(s). In the univariate case, the default prior variance is 0.49, which is the weakly informative prior variance of Dittrich et al. (2017). In the multivariate case, the prior covariance matrix is the identity matrix by default.

* `postdraws` corresponds to an integer specifying the number of posterior draws after burn-in.

* `burnin` is an integer specifying the number of draws for burn-in. Burn-in refers to the process of omitting the first posterior draws of the Markov Chain Monte Carlo algorithm.

## BF
The `banam` function is usually followed up by the `BF` function to test the hypotheses of interest. A substantive overview of the usage of the BF function can be retained from the [BF-vignette](https://cran.r-project.org/web/packages/BFpack/vignettes/vignette_BFpack.html). 


## Example 1
We will base our first demonstration on the dataset `columbus` on crime from the `spdep` package. Variable descriptions can be retained from [rrdr](https://rdrr.io/rforge/spdep/man/columbus.html).

```{r loading data for example 1, results ='hide', message=FALSE}
#install.packages("spdep")
library(spdep)
data(columbus)
```

### Data preparation - Example 1
First, we compute the weight matrix `W`, representing the neighborhood ties in our spatial network. The list `col.gal.nb` includes all neighbors. With `nb2mat`, we can add spatial weights to the list.

Second, we compute the neighborhood's values for all other covariates of interest, constituting a matrix for `X`. To obtain the data in the matrix format, we use `cbind` and include a vector of one's in the first row to model the intercepts.

Third, our outcome variable `y` is specified as a vector (in this case, residential burglaries and vehicle thefts per thousand households in the neighborhood).

```{r data preparation ex. 1}
W_crime <- nb2mat(col.gal.nb) #transforming the neighborhood list into a matrix

X_crime <- cbind(1, columbus$INC, columbus$HOVAL) #covariates: income, housing value 

y_crime <- columbus$CRIME #DV: residential burglaries and vehicle thefts per thousand households in the neighborhood
```

### Specifying banam - Example 1
After specifying the variables of interest, we can directly proceed with Bayesian analysis. 
We use the `banam` function specifying the outcome vector `y`, the covariate-value matrix `X`, and the weight matrix `W` to obtain the Bayesian estimation of the network autocorrelation model. Note, that as we did not specify a prior, by default, the flat prior is used for the estimation.

```{r Bayesian estimation of a NAM, results= F, message=FALSE}
set.seed(234)
best1 <- banam(y_crime, X_crime, W_crime)
```
```{r}
summary(best1)
```

Note that the summary output of the banam corresponds to the *estimation* of the network autocorrelation model coefficients (as opposed to tested hypotheses about the magnitude of the coefficients). From this output, we can derive the estimated network autocorrelation $\rho = 0.38$ (in other words: the magnitude of the network influence on our outcome variable). Further, we obtain information on the intercept and the two regression coefficients, $\beta_{1} = -1.12$ and $\beta_{2} = -0.27$ for our covariates income and housing value.

Furthermore, the output specifies descriptive statistics for the distribution of the coefficients. For the network autocorrelation $\rho$, given the data, a posterior mean of $\rho = 0.38$ with a posterior standard deviation of 0.133 was estimated. These values correspond to a probability of 99.68% that $\rho$ is larger than zero, meaning that there likely is a positive effect of the neighborhood tie structure on the residential burglaries and vehicle thefts. The columns 2.5% and 97.5% permit the construction of a 95% credible interval: $CI = [0.11; 0.63]$, which summarizes the uncertainty in the estimation of $\rho$. In other words, there is a 95% plausibility that $\rho$ lies between 0.115 and 0.635. Note that key differences of the equal-tailed credible interval compared to the frequentist confidence interval are the slightly different interpretation and that credible intervals may not be symmetrically dispersed around the posterior mean (check the plots section for further information on that).

A similar interpretation follows for the intercept and the coefficients. We can conclude that the average income $[\beta_{1} = -1.81;  -0.43]$ and housing values $[\beta_{2} = -0.43;  -0.09]$ in a neighborhood are negatively associated with residential burglaries and vehicle thefts.


### Density plots - Example 1
We can furthermore plot the probability density functions of the network autocorrelation coefficient $\rho$ and the covariate coefficients $\beta_{1}$ and $\beta_{2}$ as follows to obtain a visual representation of the probability distribution for the coefficients of interest. When using the column index to plot the density distributions of the beta-coefficients, note, that `plot(density(best1$beta.draws[,1]))` selects the intercept! Thus, `plot(density(best1$beta.draws[,2]))` has to be executed to print $\beta_{1}$.

```{r plotting coefficients ex. 1, fig.width=5}
plot(density(best1$rho.draws), main = "Density plot of rho", xlab = "rho") #plots the probability distribution of rho
plot(density(best1$beta.draws[,1]), main = "Density plot of the intercept", xlab = "intercept") #plots the probability distribution of the intercept
plot(density(best1$beta.draws[,2]), main = expression("Density plot of β"[1]), xlab = expression("β"[1])) #plots the probability distribution of beta1
plot(density(best1$beta.draws[,3]), main = expression("Density plot of β"[2]), xlab = expression("β"[2])) #plots the probability distribution of beta2
```

### Hypotheses testing $\rho$ - Example 1
To actually test our hypotheses, we use the `BF` function (Mulder et al., 2019), specifying the previously computed Bayesian estimation results and our hypotheses. 

A small remark regarding the specification of hypotheses: "Separate constraints within a hypothesis are separated with an ampersand `&`. Hypotheses are separated using a semi-colon `;`. For example `hypothesis = "weight > height & height > 0; weight = height = 0"` implies that the first hypothesis assumes that the parameter weight is larger than the parameter height and that the parameter height is positive, and the second hypothesis assumes that the two parameters are equal to zero. Note that the first hypothesis could equivalently have been written as `weight > height > 0`." [Source: BF vignette](https://cran.r-project.org/web/packages/BFpack/vignettes/vignette_BFpack.html).

One of the main advantages of Bayesian hypothesis testing is that effect size ranges can be included in the hypotheses. In this example, the following four hypotheses were tested:

* $H_{0}: -.25 < \rho < 0$     (negative effect) 
* $H_{1}: 0 < \rho < .25$     (small effect)
* $H_{2}: .25 < \rho < .5$    (medium effect)
* $H_{3}: .5 < \rho < 1$    (large effect)

Next to the specified hypotheses, the subsequent outputs will also specify values for the "complementary hypothesis." These values then correspond to the region outside the specified hypotheses. In the scenario above, the complementary hypothesis would, thus, include the probabilities for $\rho ≤ -.25$ and $\rho ≥ 1$.

$H_{0}$ corresponds to a *precise* hypothesis, while $H_{1}$, $H_{2}$ and $H_{3}$ correspond to *interval* hypotheses.

```{r, message=FALSE, results= F, message=FALSE}
BFbest1 <- BF(best1, hypothesis="-.25 < rho < 0;0 < rho < .25; .25 < rho < .5; .5 < rho < 1")
```
```{r}
summary(BFbest1) #obtaining more extensive summary statistics
```

### Interpretation of the output - Example 1
The first table displays the posterior probabilities of the exploratory Bayesian hypothesis test. The results indicate that the posterior probabilities of a negative effect ($\rho < 0$) are 0.003, the posterior probabilities of a positive effect ($\rho > 0$) are 0.903, and the posterior probabilities of no effect ($\rho = 0$) are 0.094, assuming equal prior probabilities. These exploratory test results suggest that a positive network effect is most likely given the observed data.

The second table summarizes the confirmatory Bayesian hypothesis test results, meaning that the specified prior probabilities are taken into account. In this table, we see that $H_{3}$ (positive medium network effect) yields the highest posterior probability.

The Bayes factors in the evidence matrix specify the likelihood ratio of all pairs of competing hypotheses. For example, the Bayes factor of $H_{1}$ against $H_{2}$ is equal to 0.019 `[1,2]`. On the other hand, the Bayes factor of $H_{2}$ against $H_{1}$ is equal to 52.755 `[2,1]`.


### Hypotheses testing $\beta$ - Example 1
We can further use the banam-computed model to test hypotheses about other predictors, which we added to the model as covariates while controlling for the network effect. 
In the following example, we will demonstrate this by testing whether the average housing value's effect on crime is likely to exceed $\beta_{2} = -0.3$ as we, e.g., deem this as arbitrarily chosen cutoff-point based on practical relevance considerations. Note that if of interest, it is theoretically also possible to test hypotheses about the intercept by specifying `Intercept` with a capital letter in the hypothesis string or to test hypotheses about multiple predictors (see example 2).


```{r, results= F, message=FALSE}
# Bayesian hypothesis testing of coefficient controlling for the network's effect
Coef1 <- BF(best1, hypothesis = "beta2 < -0.3")
```
```{r}
summary(Coef1)
```

As in the $\rho$-test, we first obtain the exploratory test results, showing the probabilities for a positive, negative, or no effect. The first hypothesis $H_{1}$, corresponds to the specified in the BF-function ($\beta_{2} < -0.3$), the other hypothesis to all values outside this interval (in this case, $\beta_{2} ≥ -0.3$). Thus, we see in the table "Posterior probabilities" that the complementary hypothesis yields a higher posterior probability: That is, the test suggests that it is more likely that $\beta_{2}$ is larger than -0.3. This can also be retrieved from the Evidence matrix, which shows that the Bayes factor of $H_{1}$ against $H_{2}$ is equal to 0.624, while the Bayes factor of $H_{2}$ against $H_{1}$ yields a value of 1.602.


## Example 2: Voting data
Example 2 draws on a data set on the 1980 Presidential election results covering 3,107 US counties from Pace and Barry (1997).
The adjacency matrix `W_votes` considers the four nearest neighbors.
The logarithmized voter turnout values are stored as dependent variable in `y_votes`. `X_votes` composes the covariate matrix including the logarithmized values of the population eligible for voting, the population of 25 years and older who completed 12th grade or higher education, the number of owner-occupied housing units, and the aggregate income per county. 

```{r load data, results ='hide'}
#Data publicly available at http://www.spatial-econometrics.com/html/jplv7.zip
#data(y_votes) #Dependent variable
#data(X_votes) #Covariate matrix
#data(W_votes) #Adjacency matrix
```

### Applying banam to Spatial Durbin Models
The original author's computed a Spatial Durbin Model for the election data. The Spatial Durbin Model tests the effect of predictors in each county, along with the effect of the predictors of the neighboring counties in each county.

The Spatial Durbin Model is given as:
$$y = ρWy + α1 + Xβ + WXγ + ɛ$$

Notation:

* $\alpha$ = intercept term
*  **1** = vector of ones
* $\gamma$ = other vector of regression coefficients

We can also estimate the Spatial Durbin Model with the banam function.
First, we multiply the weight matrix times the covariate matrix to construct `WX`. Then, we create a design matrix with a first row of ones, the regular covariate matrix `X`, and the product of the weight and covariate matrix XW. This design matrix can later be added as covariate matrix `X` to the model, and the intercept term α and the coefficients `γ` will be estimated along with the other beta-coefficients.

```{r Generating the design matrix }
# compute matrix of covariates 'WX' 
XW_votes <- W_votes %*% X_votes
#create a single design matrix with a column of ones (for intercept), X, and WX.
Xdesign <- cbind(1, X_votes ,XW_votes)
```

### Specifying banam - Example 2
Subsequently, we can specify the model using `banam`. Note that the design matrix that includes XW is specified as weight matrix.

```{r BANAM estimation voting data, results= F, message=FALSE}
set.seed(345)
best2 <- banam(y = y_votes, X = Xdesign, W = W_votes)
```
```{r}
summary(best2)
```

From this output, we can derive the estimated network autocorrelation $\rho = 0.15$ (in other words: the magnitude of the network influence on our outcome variable) and its estimated posterior standard deviation SD = 0.18. There is an estimated probability of 79.64% that there is a nonzero  positive effect of the neighboring counties on the logarithmized voter turnout in Alabama. There is a 95% plausibility that the true value of $\rho$ lies between -0.22 and 0.5.

$\beta_{1}$ to $\beta_{4}$ capture the four predictors within each county: the population eligible for voting, the population of 25 years and older who completed 12th grade or higher education, the number of owner-occupied housing units, and the aggregate income per county. $\beta_{4}$ to $\beta_{8}$ capture each county's and each county's neighboring county's score on the respective covariates.

### Hypotheses-testing $\rho$ - Example 2
We can proceed with Bayesian hypothesis testing of the effect size ranges of the network autocorrelation$\rho$. We will, again, test the following hypotheses:

* $H_{0}: -.25 < \rho < 0$     (negative effect) 
* $H_{1}: 0 < \rho < .25$     (small effect)
* $H_{2}: .25 < \rho < .5$    (medium effect)
* $H_{3}: .5 < \rho < 1$    (large effect)
* Complementary hypothesis: $\rho ≤ -.25$ and $\rho ≥ 1$.

```{r Voting data - Bayesian hypothesis testing, results= F, message=FALSE}
BFbest2 <- BF(best2, hypothesis="-.25 < rho < 0;0 < rho < .25; .25 < rho < .5; .5 < rho < 1")
```
```{r}
summary(BFbest2) 
```
The first table displays the posterior probabilities of the exploratory Bayesian hypothesis test. The results indicate that the posterior probabilities of a negative effect ($\rho < 0$) are 0.07, the posterior probabilities of a positive effect ($\rho > 0$) are 0.25, and the posterior probabilities of no effect ($\rho = 0$) are 0.69, assuming equal prior probabilities. These exploratory test results suggest that no network effect is most likely given the observed data.

The second table summarizes the results of the confirmatory Bayesian hypothesis test. By default, these are based on a uniform prior assigning equal probabilities to each interval, as no prior was specified in the `banam`-function. This table shows that $H_{2}$ (positive weak network effect) yields the highest posterior probability. Thus, given the data, it is most likely that there is a weak spatial effect on voter turnout in Alabama, US.


### Hypotheses-testing $\beta$ - Example 2
Unlike in Example 1, in the following example, multiple competing hypotheses will be tested simultaneously. With `banam` and `BFpack`, also multiple network autocorrelations can be tested. For further information on that, consult Dittrich et al. (2020).

More specifically, we will test:

* $H_{0}: \beta_{5} = \beta_{6} = 0$ (no effect of both predictors)
* $H_{1}: \beta_{5} < \beta_{6} = 0$ (negative effect of $\beta_{5}$, but no effect of $\beta_{6}$)
* $H_{2}: \beta_{5} < \beta_{6} < 0$ (negative effect of both predictors)
* $H_{3}: \beta_{5} = \beta_{6} < 0$ (no effect of $\beta_{5}$, but a negative effect of $\beta_{6}$)
* $H_{4}$: Complementary hypothesis covering all possible intervals that are not specified in the above-outlined hypotheses.

Note, that the hypotheses above were specified in an "abbreviated format." The `BF`-function also allows you to specify, e.g., `"beta5 < beta6 & beta6 = 0"` instead of `"beta5 < beta6 = 0"`, if this is more convenient.

```{r, results= F, message=FALSE}
# Bayesian hypothesis testing of coefficient controlling for the network's effect
Coef2 <- BF(best2, hypothesis="beta5 = beta6 = 0; beta5 < beta6 = 0;
          beta5 < beta6 < 0; beta5 = beta6 < 0")
```
```{r}
summary(Coef2) #obtaining more extensive summary statistics
```

From the output, we can then derive that in this example, $H_{4}$ yields the highest posterior probability (0.339), no effect of $\beta_{5}$, but a negative effect of $\beta_{6}$. Thus, the population of 25 years and older who completed 12th grade or higher education in a county of Alabama, as well as the population of 25 years and older who completed 12th grade or higher education in its neighboring counties, is typically associated with a lower voter turnout.


## Example 3
The third example is based on randomly generated data based on the descriptive statistics of the threatened bird data by McPherson and Nieswiadomy (2005). The first variable in the data set includes the generated percentage of threatened birds in 113 countries in the year 2000. The subsequent ten variables are other predictors, which the authors employed in their analysis. `W_birds` stores the data used for the connectivity matrix, which is based on the shared border length of neighboring countries.

The adjacency matrix was further row-standardized with the function `make.stochastic` from the `SNA`-package. Suppose the connectivity matrix is non-symmetrical, row or column standardization yield different results. If we choose to row standardize, the same weights are attached to all *outgoing* ties, capturing whom an actor is influencing (Leenders, 2002). However, when using column standardization, the assigned weights depend on the number of ties influencing the actor (Leenders, 2002).

```{r load bird data}
#uncomment after data is implemented
#data(y_birds)
#data(X_birds)
#data(W_birds)
```

### Specifying banam - Example 3

Subsequently, we fit the above-constructed parameters `y`, `X`, and `W` into the `banam` function. This time, an empirical prior is specified with `prior = "normal"`. Because we did not follow the specification of the empirical prior up by specifying a prior-mean with the `priormean`-argument of the function, the default empirical prior of 0.36 is used (see Dittrich, Leenders, & Mulder, 2017).

```{r banam ex. 3, results= F, message=FALSE}
set.seed(456)
best3 <- banam(y = y_birds, X = X_birds, W = W_birds, prior = "normal")
```

```{r}
summary(best3)
```

From this output, we can derive the estimated network autocorrelation $\rho = 0.07$ (in other words: the magnitude of the network influence on our outcome variable) and its estimated posterior standard deviation SD = 0.11. There is an estimated probability of 23.74% that the neighboring countries have a nonzero positive effect on the percentage of threatened birds. There is a 95% plausibility that the true value of rho lies between -0.28 and 0.14.

### Density plots - Example 3

In addition, we obtain estimations of the intercept, the ten covariates, and the residual variance. Their probability distributions can be plotted as follows (because of the large number of covariates, only $\rho$, the intercept, $\beta_{1}$, $\beta_{2}$, and $\sigma_{2}$ are plotted):

```{r plotting coefficients ex. 3, fig.width=5}
plot(density(best3$rho.draws), main = "Density plot of rho", xlab = "rho") #plots the probability distribution of rho
plot(density(best3$beta.draws[,1]), main = "Density plot of the intercept", xlab = "intercept") #plots the probability distribution of the intercept
plot(density(best3$beta.draws[,2]), main = expression("Density plot of β"[1]), xlab = expression("β"[1])) #plots the probability distribution of beta1
plot(density(best3$beta.draws[,3]), main = expression("Density plot of β"[2]), xlab = expression("β"[2])) #plots the probability distribution of beta2
plot(density(best3$sigma2.draws), main = expression("Density plot of the residual variance"), xlab = "Residual variance") #plots the probability distribution of the residual variance
```

### Hypothesis-testing $\rho$ - Example 3

We can proceed with Bayesian hypothesis testing of the effect size range of the network autocorrelation $\rho$. In this example, the following hypotheses are specified:

* $H_{0}: -.25 < \rho < 0$     (negative effect) 
* $H_{1}: 0 < \rho < .25$     (small effect)
* $H_{2}: .25 < \rho < .5$    (medium effect)
* $H_{3}: .5 < \rho < 1$    (large effect)
* Complementary hypothesis: $\rho ≤ -.25$ and $\rho ≥ 1$.

```{r Birds data - Bayesian hypothesis testing, results= F, message=FALSE}
BFbest3 <- BF(best3, hypothesis="-.25 < rho < 0;0 < rho < .25; .25 < rho < .5; .5 < rho < 1")
```
```{r}
summary(BFbest3) 
```

The first table displays the posterior probabilities of the exploratory Bayesian hypothesis test. The results indicate that the posterior probabilities of a negative effect ($\rho < 0$) are 0.18, the posterior probabilities of a positive effect ($\rho > 0$) are 0.05, and the posterior probabilities of no effect ($\rho = 0$) are 0.77, assuming equal prior probabilities. These exploratory test results suggest that no network effect is most likely given the observed data.

The second table summarizes the confirmatory Bayesian hypothesis test results, meaning that the specified empirical prior of a typical network effect of 0.36 is taken into account. This table shows that $H_{2}$ (positive weak network effect) yields the highest posterior probability. Thus, given the data and based on the effect sizes of the average network effect in prior research, it is most likely that the neighboring countries have a weak effect on the prevalence of threatened birds in a given country.


## References:
Bivand, R. S. & Wong, D. W. S. (2018) Comparing implementations of global and local indicators of spatial association. *TEST, 27*(3), 716-748. https://doi.org/10.1007/s11749-018-0599-x
  
Butts, C. T. (2020). *sna: Tools for Social Network Analysis. R package version 2.6*. https://CRAN.R-project.org/package=sna
  
Dittrich, D., Leenders, R.Th.A.J., & Mulder, J. (2017). Bayesian estimation of the network autocorrelation model. *Social Networks, 48*, 213–236. https://doi.org/10.1016/j.socnet.2016.09.002

Dittrich, D., Leenders, R.Th.A.J., & Mulder, J. (2020). Network autocorrelation modeling: Bayesian techniques for estimating and testing multiple network autocorrelations.*Social Methodology, 50*(1), 168-214. https://doi.org/10.1177/0081175020913899

Wickham, H., Hester J. & Chang, W. (2021). *devtools: Tools to Make Developing R Packages Easier. R package version 2.4.2*.

Leenders, R.Th.A.J.(2002). Modeling social influence through network autocorrelation: constructing the weight matrix. *Social Networks, 24*, 21-47.

McPherson, M. A., & Nieswiadomy, M. L. (2005). Environmental Kuznets curve: threatened species and spatial effects. *Ecological Economics, 55*(3), 395-407.

Mulder, J., van Lissa, C., Williams, D. R., Gu, X., Olsson-Collentine, A., Boeing-Messing, F., & Fox, J. (2019). *BFpack: Flexible Bayes Factor Testing of Scientific Expectations. R package version 0.3.2*. https://CRAN.R-project.org/package=BFpack

Pace, R. K. & R. Barry. 1997. Quick Computation of Spatial Autoregressive Estimators. *Geographical Analysis, 29*, 232-47.
